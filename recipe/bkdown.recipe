#!/usr/bin/env python
# vim:fileencoding=utf-8

from calendar import c
from email import header
from logging import root
from calibre.web.feeds.news import BasicNewsRecipe
from urllib.parse import urlparse, urlsplit, urljoin
from contextlib import closing
from lxml import etree
import re

BOOK_RULE = [{'书名': "//div[@id='bookdetail']/div[@id='info']/h1/text()",
              '作者': "//div[@id='info']/p[1]/a/text()",
              '地址': 'https://www.bixiabook6.com',
              '搜索': 'https://www.bixiabook6.com/search.php?keyword={key}',
              '目录': "//dt[contains(text(), '正文')]/following-sibling::dd/a",
              '站名': '笔下小说',
              '章节内容': "//div[@id='content']/text()",
              '简介': "//div[@id='intro']/text()"},
             {'书名': "//div[@id='info']/h1/text()",
              '作者': "//div[@id='info']/p[1]/text()",
              '地址': 'https://www.douyinxs.com',
              '目录': "//dt[contains(text(), '正文')]/following-sibling::dd/a",
              '站名': '抖音小说',
              '章节内容': "//div[@id='content']/text()",
              '简介': "//div[@id='intro']/text()"},
             {'书名': "//div[@class='d_title']/h1/text()",
              '作者': "//div[@class='d_title']/span/text()",
              '地址': 'https://www.uuks5.com',
              '目录': "//li[@class='lazyrender']/a",
              '站名': 'UU看书',
              '章节内容': "//div[@id='TextContent']/p/text()",
              '简介': "//div[@id='bookintro']/p/text()"},
             {'书名': "//div[@id='info']/h1/text()",
              '作者': "//div[@id='info']/p[1]/text()",
              '地址': 'https://www.biqige.info',
              '目录': "//dt[contains(text(), '正文')]/following-sibling::dd/a",
              '站名': '笔趣阁',
              '章节内容': "//div[@id='content']//text()",
              '简介': "//div[@id='intro']/p/text()",
              '编码': 'gbk'},
             {'书名': "//div[@id='info']/h1/text()",
              '作者': "//div[@id='info']/p[1]/text()",
              '地址': 'https://www.beqege.cc',
              '目录': "//dt[contains(text(), '正文')]/following-sibling::dd/a",
              '站名': '笔趣阁',
              '章节内容': "//div[@id='content']/p/text()",
              '简介': "//div[@id='intro']/p[1]/text()"},
             {'书名': "//div[@class='m-infos']/h1/text()",
              '作者': "//div[@class='m-infos']/p[1]/text()",
              '地址': 'https://www.shuao.org',
              '目录': "//div[@id='play_0']/ul/li/a",
              '站名': '盗墓小说网',
              '章节内容': ["//div[@id='content']/p/text()", [2, None]],
              '简介': "//div[@class='m-book_info']/p[1]/text()"},
             {'站名': 'uu看书网',
              '地址': 'https://www.uuks.org',
              '书名': "//div[@class='jieshao_content']/h1/a/text()",
              '封面': ["//a[@class='bookImg']/img", 'src'],
              '作者': ["//div[@class='jieshao_content']/h2/text()", [3, None]],
              '简介': ["//div[@class='jieshao_content']/h3/text()", [3, None]],
              '目录': "//div[@class='box_con']/li/a",
              '章节内容': ["//div[@id='contentbox']/p/text()", [1, None]]},
             {'书名': "//div[@class='itemtxt']/h1/a/text()",
              '作者': ["//div[@class='itemtxt']/p[2]/a/text()",  [3, None]],
              '地址': 'https://www.sudugu.com',
              '目录': "//div[@id='list']/ul/li/a",
              '封面': ["//div[@class='item']/a/img", 'src'],
              '站名': '速读网',
              '章节内容': ["//div[@class='con']/p/text()", [2, None]],
              '简介': "//div[@class='des bb']/p/text()"}]
parser = etree.HTMLParser()


class BookDown(BasicNewsRecipe):
    title = '网络小说'
    __author__ = '书虫'
    description = '小说'
    url = "https://www.sudugu.com/321/"
    language = 'zh_CN'
    oldest_article = 7
    articles_are_obfuscated = True
    max_articles_per_feed = 100000

    def __init__(self, options, log, progress_reporter):
        super().__init__(options, log, progress_reporter)
        self.rule = self._getbookrule(self.url)
        self.logger = log
        self.logger.info(f"即将依据{self.rule['站名']}的信息爬取")
        self.encoding = self.rule.get('编码', 'utf-8')

    def _getbookrule(self, url):
        urlsp = urlparse(url)
        now_rule = None
        if not (urlsp.scheme and urlsp.netloc):
            raise ValueError(f"{url}错误，请输入完整的url地址")
        for rule in BOOK_RULE:
            if rule["地址"] in url:
                now_rule = rule
                break
        if now_rule is None:
            self.logger.info(f"未能找到{url}的相关规则，"
                             "请先更新规则或者检查url")
            raise ValueError(f"{url}未能匹配规则")
        return now_rule

    def get_url_specific_delay(self, url):
        import random
        return random.random()*self.timeout

    def get_dat(self, root, nkey):
        '''
        root, nkey为规则名
        规则为：xpath路径(str), 切片/索引
        或者， xpath路径, 属性名(str)
        或者， xpath路径, 属性名, 切片/索引
        '''
        rule = self.rule[nkey]
        self.logger.debug(f"{nkey}={rule}")
        if not isinstance(rule, str):
            rule, *value = rule
        else:
            value = []
        dat = root.xpath(rule)
        if nkey not in ['目录', '章节内容']:
            dat = dat[0]
        if not value:
            return dat
        if isinstance(value[0], str):
            dat = dat.attrib[value[0]]
            try:
                if isinstance(value[1], int):
                    dat = dat[value[1]]
                if isinstance(value[1], list):
                    sl = slice(*value[1])
                    dat = dat[sl]
            except IndexError:
                pass
        else:
            if isinstance(value[0], int):
                dat = dat[value[0]]
            if isinstance(value[0], list):
                sl = slice(*value[0])
                dat = dat[sl]

        self.logger.debug(f"{nkey}={dat}")
        return dat

    def get_browser(self, *args, **kwargs):
        br = super().get_browser(*args, **kwargs)
        try:
            headers = self.rule.get('headers', {})
        except AttributeError:
            self.rule = self._getbookrule(self.url)
            headers = self.rule.get('headers', {})
        for k, v in headers.items():
            br.set_current_header(k, v)
        return br

    def page_to_soup(self, url):
        parser = etree.HTMLParser()

        br = self.get_browser()
        with closing(br.open_novisit(url, timeout=self.timeout)) as f:
            raw = f.read()
        content = raw.decode(self.encoding)
        root = etree.fromstring(content, parser=parser)
        return root

    def parse_index(self):
        if not all([self.url, self.rule]):
            raise ValueError(f"当前规则不支持该链接：{self.url}")

        root = self.page_to_soup(self.url)
        bk = {}
        bk["书名"] = self.get_dat(root, "书名")
        self.title = bk["书名"]
        bk["作者"] = self.get_dat(root, "作者")
        bk["作者"] = re.split(r":|：", bk["作者"])[-1]
        self._BasicNewsRecipe__author__ = bk["作者"]
        bk["简介"] = self.get_dat(root, "简介")
        self.description = bk["简介"]
        self.cover_url = urljoin(self.url, self.get_dat(root, "封面"))
        self.logger.debug(f"书名={bk['书名']}")
        self.logger.debug(f"作者={bk['作者']}")
        self.logger.debug(f"简介={bk['简介']}")

        mulu_url = self.rule.get("目录页", None)
        if mulu_url:
            mulu_url = self.get_dat(root, "目录页")
            mulu = self.get_mulu(mulu_url)
        else:
            mulu_url = self.url
            mulu = self.get_mulu(root)
        self.logger.debug(f"本书{bk['书名']}共有{len(mulu)}个章节")
        return [(bk['书名'], mulu)]

    def get_mulu(self, mmm):
        '''
        获取目录
        '''
        if isinstance(mmm, str):
            root = self.page_to_soup(mmm)
        else:
            root = mmm
        mulu = self.get_dat(root, "目录")
        mulu_dats = []
        for ii, m in enumerate(mulu):
            mtitle = m.text
            murl = urljoin(self.url, m.attrib['href'])
            mulu_dats.append({"id": ii, "title": mtitle, "url": murl})
        nxt_url = root.xpath("//a[contains(text(), '下一页')]")
        if nxt_url:
            nxt_url = urljoin(self.url, nxt_url[0].attrib['href'])
            mulu_list += self.mulu(nxt_url)
        self.logger.debug(f"共读取到{len(mulu_dats)}条目录")
        return mulu_dats

    def get_obfuscated_article(self, url):
        '''
        获取章节内容
        '''
        self.logger.debug(f"获取章节内容：{url}")
        content = self.zhangjie_data(url)
        # content = "\n".join(content)
        # content = "<br>".join(content)
        return {'url': url, 'data': content}

    def zhangjie_data(self, url):
        zj = []
        root = self.page_to_soup(url)
        # zj = root.xpath(self.now_rule["xpath"]["章节内容"])
        zj = self.get_dat(root, "章节内容")
        zj = [x.strip() for x in zj if x.strip()]
        zj = ["<p>%s</p>" % x for x in zj]
        nxt_url = root.xpath("//a[contains(text(), '下一页')]")
        if nxt_url:
            nxt_url = urljoin(url, nxt_url[0].attrib['href'])
            zj += self.zhangjie_data(nxt_url)

        return zj
